{"name":"CS5395 - Twitter EDA pipeline","tagline":"CS Independent Study Project - Texas State University","body":"### Tweet Analysis DB\r\nThis repository was created as part of CS5395 IND STUDY Project at Texas State University, Computer Science Department.\r\n\r\nStudent: Lia Nogueira de Moura /  Advisor: Dr. Jelena Tesic\r\n\r\n \r\n\r\n### Abstract\r\nTwitter data has been used extensively for research in the past years. The large number of users and messages shared every day creates interesting data for many different areas of research. The process to analyze Twitter data requires a series of initial steps independent of the research subject. This study proposes a subject agnostic pipeline to analyze Twitter data. It also includes an exploratory topic analysis using three known unsupervised machine learning models (LDA, LSI and NFM).  A database of over 3 million tweets related to the #MeToo movement was used as case study. Analytics extracted from this dataset illustrate common findings and challenges.\r\n\r\n\r\n#### Database Design\r\nDocument driven NoSql databases such as MongoDB use the concept of collections. Collections are structures that hold all documents. Collections are similar to tables in relational databases. \r\nThe following are the collections created for this pipeline:\r\n\r\n-dbSettings: This collection will contain the basic configuration to run the program. This is to make sure each person only uses what it is important to their project. (e.g. no reason to save certain fields like geocode if they are not important for the analysis.) This table will be used as a driver to decide what to save on the database.\r\n\r\n-loadedFiles: This collection will save the directory and file names that have been loaded already. LoadTime and path will be columns. This is to make sure we don't load the same files multiple times.\r\ntwitterSearches. This collection will save the searches requested to Twitter API. This will only be used when the tweets are being saved straight from Twitter API.\r\nThis is to keep track of what search requests have been done already. It will help to determine the number of results returned for a search and will also help us to make sure we don’t waste requests doing the same searches.\r\n\r\n-tweet: This collection will be used to save the complete tweet document. It will also contain a sequence number that will be used in the recovery process.\r\n\r\n-focusedTweet: This collection will only contain the interesting information from the tweets. It will only contain the fields that were set on the settings. \r\n\r\n-tweetWords: This collection will save each word separately from every tweet plus some extra information about the tweet. It will also contain interesting tags about that word. (e.g English or not, verb or not, etc.)\r\n\r\n-tweetSentences: This collection will save each sentence separately from every tweet plus some extra information about the tweet. It will also contain interesting tags about that word. \r\n\r\n-topicsByHashTag: This collection will save topic information for each hashtag using the three different unsupervised ML models\r\n\r\n-loadStatus: This collection will controls have been loaded into DB already. This collection was created to manage the recovery logic.\r\n\r\n-tweetCountByFileAgg: This collection will contain the count of all tweets in the dataset by files loaded.\r\n\r\n-tweetCountByPeriodAgg: This collection will contain the count of all tweets in the dataset by period. The period will be specified in the settings collection.\r\n\r\n-tweetCountByLanguageAgg: This collection will contain the count of all tweets in the dataset by language.\r\n\r\n-tweetCountByUserAgg: This collection will contain the count of all tweets in the dataset by user id. It will also include details about the user itself. The extra information about the user loaded in this collection will be identified in the settings collection.\r\n\r\n-wordCountAgg: This collection will contain the frequency of all words in the dataset.\r\n\r\n-hashTagCountAgg: This collection will contain the frequency of all hashtags in the dataset.\r\n\r\n-userLocationCountAgg: This collection will contain the count of all users in the dataset by their location\r\n\r\n\r\n\r\n\r\n#### Python Library\r\nThe library includes logic to extract data directly from Twitter’s API and to extract data from existent twitter .json files. The library also contains a process to select, clean and organize the data based on the given settings. Logic to export meaningful data to tab delimited .txt files is also included.\r\n\r\nThe following are the main functions included in the library:\r\n\r\n-loadSettings: This method will load the settings saved in the DB into class variables\r\n\r\n-updateSettings: This method will update the setting into DB. It will also update the class variables \r\n\r\n-loadDocFromFile: This method will load tweet .json files into the DB (tweet collection). It goes through all .json files in the directory and load them one by one. It also saves the files already loaded into the 'loadedFiles' collection to make sure we don't load the same file twice.\r\n\r\n-extractDocFromAPI: This method will load tweets from Twitter API \r\n\r\n-loadFocusedData: This method will load the focusedTweet collection with the interesting information we want to study \r\n\r\n-breakTextIntoWords: This method will break text from tweets into words and tag them\r\n\r\n-breakTextIntoSentences: This method will break text from tweets into sentences\r\n\r\n-findTopicByHashtag: This method will find the topics for a hashtag using different models and tools\r\n\r\n-loadAggregations: This method will load the aggregation queries. One or many can be run. The parameters will help decide which ones to run.\r\n\r\n-exportData: This method will export the data into “|” delimited files\r\n\r\n\r\n#### Recovery Process\r\nThe pipeline includes a recovery logic to make sure the processes don’t have to get run from the beginning in case of a failure. While inserting into the tweet collection, a sequence number gets attached to each tweet. Then every time any processes need to run, that sequence number is used to control what has already been processed or not. If something fails, the logic will be able to identify the last sequence number processed and continue from there.\r\n\r\n","note":"Don't delete this file! It's used internally to help with page regeneration."}